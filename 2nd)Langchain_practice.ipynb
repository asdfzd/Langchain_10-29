{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:1b', 'created_at': '2024-11-02T11:17:09.2629038Z', 'message': {'role': 'assistant', 'content': '지구의자전은 24시간에서 365天으로 1년을 구성하고, 1주기에는 360도까지 자전한다.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 18019647600, 'load_duration': 7990671300, 'prompt_eval_count': 33, 'prompt_eval_duration': 2032043000, 'eval_count': 35, 'eval_duration': 7975211000}\n"
     ]
    }
   ],
   "source": [
    "# ollama 라이브러리를 사용하기 위해 불러옴\n",
    "import ollama\n",
    "\n",
    "# ollama의 'chat' 기능을 사용하여 'llama3.2:1b' 모델로 대화 생성\n",
    "response = ollama.chat(model='llama3.2:1b', messages=[\n",
    "    {\n",
    "        'role': 'user',  # 대화에서 사용자 역할 지정\n",
    "        'content': '지구의 자전 주기는?'  # 질문 내용\n",
    "    },\n",
    "])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지구의 자전 주기는 약 26,000 세가년입니다.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama3.2:1b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '지구의 자전 주기는?'\n",
    "    },\n",
    "])\n",
    "\n",
    "# response에서 전체 응답 중 'message'와 'content' 키 값만 추출하여 출력\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지구의 자전 주기에는 365.24219天이 있으며, 이 때earth가 반도와 함께 1년을 지속하고, 1년이 지속되기 때문에 1년이 365.24219*365.24219天으로 calculate 되고 있다.\n",
      "\n",
      "이러한 이유로 earth가 1년이 지속되지만, 1년이 지속되는지에 대한 질문에 대하여, 이 개념은 천문학에서 자전 주기라고 하는데, 그 purpose는 simply 1년을 지속하고, earth와 반도와의 close distance를 유지하기 위해서이다.\n",
      "\n",
      "자전 주기는 actually 365.24219*365.24219天으로 calculate 되고 있다.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# 'chat_with_llama' 함수 정의 - 사용자 입력(prompt)에 따라 모델의 응답을 반환\n",
    "def chat_with_llama(prompt):\n",
    "    response = ollama.chat(model='llama3.2:1b', messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "        },\n",
    "    ])\n",
    "    return response['message']['content']  # 응답 중 'message'와 'content' 키 값만 반환\n",
    "\n",
    "# 사용자 입력과 답변 가이드를 결합하여 프롬프트 생성\n",
    "prompt = \"\"\"\n",
    "지구의 자전 주기를 알려줘.\n",
    "\"\"\"\n",
    "\n",
    "guide = \"\"\"\n",
    "너는 천문학자야. 너는 답변을 천문학자처럼 전문적으로 답변해야돼.\n",
    "아래 인풋을 답변해줘.\n",
    "인풋 :\n",
    "\"\"\"\n",
    "\n",
    "prompt = guide + prompt + \"\\n    답변 : \"  # 가이드와 질문을 하나의 프롬프트로 결합\n",
    "\n",
    "output = chat_with_llama(prompt)  # 모델 응답 호출 후 출력\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# 'chat_with_llama' 함수 정의 - 사용자 입력(prompt)에 따라 모델의 응답을 반환\n",
    "def chat_with_llama(prompt):\n",
    "    response = ollama.chat(model='llama3.2:1b', messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "        },\n",
    "    ])\n",
    "    return response['message']['content']  # 응답 중 'message'와 'content' 키 값만 반환\n",
    "\n",
    "# 사용자 입력과 답변 가이드를 결합하여 프롬프트 생성\n",
    "prompt = \"\"\"\n",
    "지구\n",
    "\"\"\"\n",
    "\n",
    "guide = \"\"\"\n",
    "너는 번역기야. 인풋을 영어로 번역해줘.\n",
    "\n",
    "인풋 : 미래\n",
    "답변 : future\n",
    "\n",
    "인풋 : 결과\n",
    "답변 : result \n",
    "\n",
    "답변은 반드시 인풋에 대한 번역만 알려주고 다른 글자는 출력해선 안돼.\n",
    "\n",
    "그럼 아래 인풋을 답변해줘.\n",
    "\n",
    "인풋 : \n",
    "\"\"\"\n",
    "\n",
    "prompt = guide + prompt + \"\\n    답변 : \"  # 가이드와 질문을 하나의 프롬프트로 결합\n",
    "\n",
    "output = chat_with_llama(prompt)  # 모델 응답 호출 후 출력\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프롬프트 작성 요령\n",
    "1. 명확성과 구체성(예시: 다음 주 주식 시장에 영향을 줄 수 있는 예정된 이벤트들은 무엇일까요?)\n",
    "2. 배경 정보를 포함 (예시: 2020년 미국 대선의 결과를 바탕으로 현재 정치 상황에 대한 분석을 해주세요.)\n",
    "3. 간결함 (예시:2021년에 발표된 삼성전자의 ESG 보고서를 요약해주세요.)\n",
    "4. 열린 질문 사용(예시:신재생에너지에 대한 최신 연구 동향은 무엇인가요?)\n",
    "5. 명확한 목표 설정(예시:AI 윤리에 대한 문제점과 해결 방안을 요약하여 설명해주세요.\n",
    "6. 언어와 문체(예시:공식적인 보고서를 요청하는 경우, \"XX 보고서에 대한 전문적인 요약을 부탁드립니다.\"와 같이 정중한 문체를 사용합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='지구의 자전 주기는 약 365.242198 일로 determined, 이는 지구가 연간 한 일과를 의미한다. 이 일은 지구가 rotates하여 바다에 들어가는 시간을 말하며, 지구가 태양이 보면서Rotate되기 때문이다.' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2024-11-01T17:51:39.449705Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 11226713900, 'load_duration': 65029900, 'prompt_eval_count': 48, 'prompt_eval_duration': 187262000, 'eval_count': 64, 'eval_duration': 10971210000} id='run-86124c5d-ddd1-4fcd-b259-d7dedcecb063-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama \n",
    "# ChatOllama 모델을 사용하기 위해 불러옴\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# 프롬프트 템플릿을 생성하기 위해 불러옴\n",
    "\n",
    "# 프롬프트 템플릿 생성 - 천문학 전문가로서 질문에 답변하도록 설정\n",
    "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question> : {input}\")\n",
    "\n",
    "# llama3.2:1b 모델로 설정하여 ChatOllama 인스턴스 생성\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "# 프롬프트와 모델을 결합한 체인 생성\n",
    "chain = prompt | llm\n",
    "\n",
    "\n",
    "# 체인을 통해 프롬프트 입력에 따라 모델을 호출하여 응답 생성\n",
    "output = chain.invoke({\"input\": \"지구의 자전 주기는?\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지구의 자전 주기는 약 365.242199 일이며, 이란다에서 태어난 프랑스 전사인 Copernicus에 의해 처음 정리되어 왔다.이 수치는 현재 대도시지의 시대에 사용할 수 있는 유일한 지구 자전 주기를 대체한다.\n",
      "\n",
      "1년은 지구가 rotates 360도를 돌아가는 반면, 지구가 rotate 한 경계에서 항상 바닥을 보는 경향이 있기 때문에 일 년은 지구가 360 도를 rotate 하면서 볼 수 있는 경계로 볼 때까지 지동하는 방식에서 1년의 자전 주기는 지구가 rotate 한 경계에서 항상 바닥을 보는 방식으로 24시간에서 1일로 변환된다.\n",
      "\n",
      "이란다에서 태어난 Copernicus에 의해 정리되어 있는 지구의자전 주기는 약 365.242199 일이며, 이 수치는 현재 대도시지의 시대에 사용할 수 있는 유일한 지구 자전 주기를 대체한다.\n",
      "\n",
      "이와 같이, 1년은 지구가 rotate 한 경계에서 항상 바닥을 보는 경향이 있기 때문에 일 년은 지동하는 방식으로 24시간에서 1일로 변환된다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama  # ChatOllama 모델을 사용하기 위해 불러옴\n",
    "from langchain_core.prompts import ChatPromptTemplate  # 프롬프트 템플릿을 생성하기 위해 불러옴\n",
    "from langchain_core.output_parsers import StrOutputParser  # 문자열 형식의 출력을 위한 파서 불러옴\n",
    "\n",
    "# 프롬프트 템플릿 생성 - 천문학 전문가로서 질문에 답변하도록 설정\n",
    "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question> : {input}\")\n",
    "\n",
    "# llama3.2:1b 모델로 설정하여 ChatOllama 인스턴스 생성\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "# 출력 파서를 문자열로 설정\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 출력 파서를 추가하여 최종 체인 생성\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "\n",
    "\n",
    "# 체인을 통해 프롬프트 입력에 따라 모델을 호출하여 응답 생성\n",
    "output = chain.invoke({\"input\": \"지구의 자전 주기는?\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain1 Answer\n",
      "\n",
      "The translation of \"미래\" into English is:\n",
      "\n",
      "Future\n",
      "chain2 Answer\n",
      "In Korean, \"미래\" (mitara) can indeed be translated to English as \"future\", \"tomorrow\", or even \"destiny\". However, there's another way to express the idea, which is more neutral and less specific.\n",
      "\n",
      "If you want a more neutral translation, you could use \"\" (jipeun manneun), which literally means \"a time ahead of us\" or \"upcoming times\". This phrase can convey a sense of something being expected or anticipated in the future without implying a specific destination or goal.\n",
      "\n",
      "To give you an example, if someone mentions that they have plans for next year or plan to work on a project soon, they might say \"\" (jipeun manneun) instead of \"the future\".\n",
      "\n",
      "It's worth noting that Korean is a language with many nuances and complexities, and the word choices can vary depending on the context and tone you want to convey.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "# ChatOllama 모델을 사용하기 위해 불러옴\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# 프롬프트 템플릿을 생성하기 위해 불러옴\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# 문자열 형식으로 출력을 변환하기 위한 파서 불러옴\n",
    "\n",
    "# 첫 번째 프롬프트 템플릿 생성 - 한국어 단어를 영어로 번역 요청\n",
    "prompt1 = ChatPromptTemplate.from_template(\"translate {korean_word} to English\")\n",
    "\n",
    "# 두 번째 프롬프트 템플릿 생성 - 영어 단어를 한국어로 설명 요청\n",
    "prompt2 = ChatPromptTemplate.from_template(\"explain {english_word} using Oxford dictionary to me in Korean\")\n",
    "\n",
    "# llama3.2:1b 모델로 설정하여 ChatOllama 인스턴스 생성\n",
    "llm = ChatOllama(model=\"llama3.2.:1b\")\n",
    "\n",
    "# 출력 파서를 문자열 형식으로 설정\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 첫 번째 체인 생성 - prompt1과 llm을 결합하고 output_parser를 통해 결과를 문자열로 출력\n",
    "chain1 = prompt1 | llm | output_parser\n",
    "\n",
    "chain2 = (\n",
    "    {\"english_word\": chain1}\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | output_parser\n",
    "\n",
    ")\n",
    "# 체인을 통해 프롬프트 입력에 따라 모델을 호출하여 응답 생성\n",
    "print(\"chain1 Answer\\n\")\n",
    "output = chain1.invoke({\"korean_word\": \"미래\"})\n",
    "print(output)\n",
    "\n",
    "print(\"chain2 Answer\")\n",
    "output2 = chain2.invoke({\"korean_word\" : \"미래\"})\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터 조정\n",
    "\n",
    "Temperature: 생성된 텍스트의 다양성 조정. 값이 작으면 예측 가능하고 일관된 출력을 생성하는 반면, 값이 크면 다양하고 예측하기 어려운 출력 생성.(0~2)\n",
    "\n",
    "Max Tokens: 생성할 최대 토큰 수 지정.\n",
    "\n",
    "Frequency Penalty: 값이 클수록 이미 등장한 단어나 구절이 다시 등장할 확률을 감소시킨다. 이를 통해 반복을 줄이고 다양성을 늘린다(0~1)\n",
    "\n",
    "Present Penalty: 텍스트 내에서 단어의 존재 유무에 따라 그 단어의 선택 확률을 조정한다. 값이 클수록 아직 텍스트에 등장하지 않은 새로운 단어 사용이 장려된다.(0~1)\n",
    "\n",
    "Stop Sequences: 특정 단어나 구절이 등장할 경우 생성이 멈추도록 설정.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "태양계에서 가장 큰 행성은 태양입니다. 태양은 대형 행성 중 하나로, 태양계의 가장 큰 행성 중 하나입니다. 태양은 태양계의 주인공으로, 태양의 rotating 지구에 있는 물질과 전기가 flows 하며, 그로 인해 태양계의 climate와 지역별 기온 차이를 tạo합니다.\n",
      "\n",
      "태양은 태양계에서 가장 큰 행성이ですが, 대체로 태양계를 공유하는 주인공이 되지 못하고 있다. 태양은 태양계의 주인공으로, 태양계에 있는 물질과 전기와의 상호 작용이 주로 이루られます.\n",
      "\n",
      "태양은 태양계에서 가장 큰 행성이ですが, 대체로 태양계를 공유하는 주인공이 되지 못하고 있다. 태양은 태양계의 주인공으로, 태양계에 있는 물질과 전기와의 상호 작용이 주로 이루られます.\n",
      "\n",
      "태양은 태양계에서 가장 큰 행성이ですが, 대체로 태양계를 공유하는 주인공이 되지 못하고 있다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama  # ChatOllama 모델을 사용하기 위해 불러옴\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "params = {\n",
    "    \"temperature\": 0.7,  # 생성된 텍스트의 다양성을 조정\n",
    "    \"max_tokens\": 100,   # 생성할 최대 토큰 수\n",
    "}\n",
    "\n",
    "# 추가 설정 (kwargs) 정의\n",
    "kwargs = {\n",
    "    \"frequency_penalty\": 0.5,  # 이미 등장한 단어의 재등장 확률 감소\n",
    "    \"presence_penalty\": 0.5,   # 새로운 단어의 도입을 장려\n",
    "    \"stop\": [\"\\n\"]             # 정지 시퀀스 설정\n",
    "}\n",
    "\n",
    "# ChatOllama 모델 인스턴스 생성, params와 kwargs를 사용하여 설정 적용\n",
    "model = ChatOllama(model=\"llama3.2:1b\", **params, model_kwargs=kwargs)\n",
    "\n",
    "# 질문 설정\n",
    "question = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n",
    "\n",
    "# max_tokens를 10으로 설정하여 모델에 질문을 전달하고 응답 생성\n",
    "response = model.invoke(input=question, max_tokens=10)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)  # 응답의 텍스트 콘텐츠만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자열 템플릿\n",
    "PromptTemplate.from_template 메소드를 사용하여 문자열 템플릿으로부터 PromptTemaplte인스턴스를 생성한다. 이때, template_text 변수에 정의된 템플릿 문자열이 사용된다.\n",
    "\n",
    "생성된 PromptTemplate 인스턴스의 format 메소드를 사용하여, 실제 ‘name’과 ‘age’값으로 템플릿에 채워서 프롬프트를 구성한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프롬프트 템플릿 간의 결합\n",
    "PromptTemplate 클래스는 문자열을 기반으로 프롬프트 템플릿을 생성하고, +연산자를 사용하여 직접 결합하는 동작을 지원한다.\n",
    "\n",
    "PromptTemplate 인스턴스간의 직접적인 결합뿐만 아니라, 이들 인스턴스와 문자열로 이루어진 템플릿을 결합하여 새로운 PromptTemplate 인스턴스를 생성하는 것도 가능하다.\n",
    "\n",
    "StroutputParser을 통해 문자열로 변환하는 LLM 체인을 구성한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Hong Gildong and I am 30 years old.\n",
      "\n",
      "I don't know if you'd like me to use your given name, but since we just met, I'll assume it's okay. If not, please let me know what to call you instead.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# 프롬프트 템플릿을 생성하기 위해 불러옴\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "# ChatOllama 모델을 사용하기 위해 불러옴\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# 문자열 형식으로 출력을 변환하기 위한 파서 불러옴\n",
    "\n",
    "\n",
    "template_text = \"안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다.\"\n",
    "# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_text)\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "\n",
    "filled_prompt = prompt_template.format(name=\"홍길동\", age=30)\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "# print(filled_prompt)  # 출력 예상: \"안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다.\"\n",
    "# 이 부분은 단순히 변수 값을 채운 기본 문장을 보여줌.\n",
    "\n",
    "combined_prompt = (\n",
    "    prompt_template\n",
    "    + ChatPromptTemplate.from_template(\"\\n\\n아버지를 아버지라 부를 수 없습니다.\")\n",
    "    + \"\\n\\n{language}로 번역해주세요.\"\n",
    ")\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "# print(combined_prompt)  # 출력 예상: \"안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다.\\n\\n아버지를 아버지라 부를 수 없습니다.\\n\\n{language}로 번역해주세요.\"\n",
    "# 여러 프롬프트와 문자열이 결합된 최종 프롬프트 템플릿을 보여줌.\n",
    "\n",
    "output = combined_prompt.format(name=\"홍길동\", age=30, language=\"영어\")\n",
    "# combined_prompt에 값을 채워서 최종 출력물 생성\n",
    "# print(output)  # 출력 예상: \"안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다.\\n\\n아버지를 아버지라 부를 수 없습니다.\\n\\n영어로 번역해주세요.\"\n",
    "# 결합된 템플릿에 변수 값을 채운 최종 문장을 보여줌.\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "# 모델 생성 및 체인 연결 (llama3.1 모델 설정)\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "# combined_prompt와 모델, 파서를 결합하여 체인 생성\n",
    "\n",
    "output_2 = chain.invoke({\"age\": 30, \"language\": \"영어\", \"name\": \"홍길동\"})\n",
    "# 체인을 통해 입력 값에 따라 모델을 호출하여 응답 생성\n",
    "print(output_2)\n",
    "# 최종 응답 출력. 모델 응답은 '홍길동'의 나이와 관련된 내용을 영어로 번역하는 답변이 될 것으로 예상됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챗 프롬프트 템플릿\n",
    "ChatPromptTemplate은 대화형 상황에서 여러 메시지 입력을 기반으로 단일 메시지 응답을 생성하는 데 사용됩니다. 이는 대화형 모델이나 챗봇 개발에 주로 사용됩니다. 입력은 여러 메시지를 원소로 갖는 리스트로 구성되며, 각 메시지는 역할(role)과 내용(content)으로 구성됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Message 유형"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
